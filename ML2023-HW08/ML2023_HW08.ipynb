{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":51688,"databundleVersionId":5521765,"sourceType":"competition"}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 8 - Anomaly Detection**\n\nIf there are any questions, please contact mlta-2023-spring@googlegroups.com\n\nSlide:    [Link](https://docs.google.com/presentation/d/18LkR8qulwSbi3SVoLl1XNNGjQQ_qczs_35lrJWOmHCk/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/t/c76950cc460140eba30a576ca7668d28)","metadata":{"id":"YiVfKn-6tXz8"}},{"cell_type":"markdown","source":"# Set up the environment\n","metadata":{"id":"bDk9r2YOcDc9"}},{"cell_type":"markdown","source":"## Package installation","metadata":{"id":"Oi12tJMYWi0Q"}},{"cell_type":"code","source":"# Training progress bar\n!pip install -q qqdm","metadata":{"id":"7LexxyPWWjJB","outputId":"2039780b-43f2-49b5-ffa8-2a0c33ad2d15","execution":{"iopub.status.busy":"2024-12-24T12:02:58.031105Z","iopub.execute_input":"2024-12-24T12:02:58.031330Z","iopub.status.idle":"2024-12-24T12:03:09.542930Z","shell.execute_reply.started":"2024-12-24T12:02:58.031307Z","shell.execute_reply":"2024-12-24T12:03:09.541688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Downloading data","metadata":{"id":"DCgNXSsEWuY7"}},{"cell_type":"code","source":"!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n!apt-get install -y --allow-unauthenticated git-lfs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:03:09.545317Z","iopub.execute_input":"2024-12-24T12:03:09.545789Z","iopub.status.idle":"2024-12-24T12:03:29.540704Z","shell.execute_reply.started":"2024-12-24T12:03:09.545748Z","shell.execute_reply":"2024-12-24T12:03:29.539681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/chiyuanhsiao/ml2023spring-hw8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:03:29.542106Z","iopub.execute_input":"2024-12-24T12:03:29.542377Z","iopub.status.idle":"2024-12-24T12:03:30.796818Z","shell.execute_reply.started":"2024-12-24T12:03:29.542350Z","shell.execute_reply":"2024-12-24T12:03:30.795581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/ml2023spring-hw8\n!git lfs install\n!git lfs pull","metadata":{"id":"e-yCMrIl4L60","outputId":"7a5c329a-933e-4564-8423-b11469700577","execution":{"iopub.status.busy":"2024-12-24T12:03:30.798351Z","iopub.execute_input":"2024-12-24T12:03:30.798669Z","iopub.status.idle":"2024-12-24T12:04:14.492661Z","shell.execute_reply.started":"2024-12-24T12:03:30.798628Z","shell.execute_reply":"2024-12-24T12:04:14.491396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import packages","metadata":{"id":"HNe7QU7n7cqh"}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\nfrom torch.optim import Adam, AdamW\nfrom qqdm import qqdm, format_str\nimport pandas as pd\nfrom torchinfo import summary","metadata":{"id":"Jk3qFK_a7k8P","execution":{"iopub.status.busy":"2024-12-24T12:04:14.494210Z","iopub.execute_input":"2024-12-24T12:04:14.494498Z","iopub.status.idle":"2024-12-24T12:04:17.102494Z","shell.execute_reply.started":"2024-12-24T12:04:14.494469Z","shell.execute_reply":"2024-12-24T12:04:17.101657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading data","metadata":{"id":"6X6fkGPnYyaF"}},{"cell_type":"code","source":"train = np.load('/kaggle/working/ml2023spring-hw8/trainingset.npy', allow_pickle=True)\ntest = np.load('/kaggle/working/ml2023spring-hw8/testingset.npy', allow_pickle=True)\n\nprint(train.shape)\nprint(test.shape)","metadata":{"id":"k7Wd4yiUYzAm","outputId":"d8b80f1e-cbc4-4ed9-c95c-b2e3fb561f23","execution":{"iopub.status.busy":"2024-12-24T12:04:17.103556Z","iopub.execute_input":"2024-12-24T12:04:17.104062Z","iopub.status.idle":"2024-12-24T12:04:17.679079Z","shell.execute_reply.started":"2024-12-24T12:04:17.104021Z","shell.execute_reply":"2024-12-24T12:04:17.678092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random seed\nSet the random seed to a certain value for reproducibility.","metadata":{"id":"_flpmj6OYIa6"}},{"cell_type":"code","source":"def same_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nsame_seeds(48763)","metadata":{"id":"Gb-dgXQYYI2Q","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:04:17.681382Z","iopub.execute_input":"2024-12-24T12:04:17.681703Z","iopub.status.idle":"2024-12-24T12:04:17.735482Z","shell.execute_reply.started":"2024-12-24T12:04:17.681673Z","shell.execute_reply":"2024-12-24T12:04:17.734741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Autoencoder","metadata":{"id":"zR9zC0_Df-CR"}},{"cell_type":"markdown","source":"# Models & loss","metadata":{"id":"1EbfwRREhA7c"}},{"cell_type":"code","source":"class Residual_block(nn.Module):\n    def __init__(self, ic, oc, stride=1): # 当传入 stride = 2 时，会把图片长宽缩小一倍\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(ic, oc, stride=stride, padding=1, kernel_size=3), # stride=2: (H,W) -> (H/2, W/2)\n            nn.BatchNorm2d(oc),\n            nn.ReLU(inplace=True)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(oc, oc, stride=1, padding=1, kernel_size=3), # (H,W) -> (H,W)\n            nn.BatchNorm2d(oc)\n        )\n        \n        self.downsample = None # 让原来的 x 变成能和 forward(x) 相加的形状，包括 channel 和 (H,W) 都应相同\n        if((stride != 1)  or (ic != oc)): # stride != 1 -> (H,W) 变小 ic != oc -> channel 不同\n            self.downsample = nn.Sequential(\n                nn.Conv2d(ic, oc, stride=stride, kernel_size=1),\n                nn.BatchNorm2d(oc)\n            )\n            \n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        \n        if(self.downsample != None):\n            residual = self.downsample(residual)\n        \n        x = x + residual\n        x = nn.ReLU(inplace = True)(x)\n        return x\n\nclass ResNet(nn.Module):\n    def __init__(self, block=Residual_block, num_layers = [2, 1, 1, 1]):\n        super().__init__()\n        self.preconv = nn.Sequential( # 3*64*64 --> 32*64*64\n            nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n        \n        def make_residual(block, ic, oc, num_layer, stride=1):\n            layers = []\n            layers.append(block(ic, oc, stride))\n            for i in range(num_layer-1):\n                layers.append(block(oc, oc))\n            return nn.Sequential(*layers)\n\n        self.layer0 = make_residual(block, ic=32, oc=64, num_layer=num_layers[0], stride=2)\n        self.layer1 = make_residual(block, ic=64, oc=128, num_layer=num_layers[1], stride=2)\n        self.layer2 = make_residual(block, ic=128, oc=128, num_layer=num_layers[2], stride=2)\n        self.layer3 = make_residual(block, ic=128, oc=64, num_layer=num_layers[3], stride=2) \n\n        self.fc = nn.Sequential(\n            nn.Flatten(), # 也可以用 .view(shape[0], -1)\n            nn.Dropout(0.2),\n            nn.Linear(64*4*4, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace = True)\n        )\n        \n        # 关于 ConvTranspose2d 的介绍：https://blog.csdn.net/qq_36201400/article/details/112604740\n        # 大概就是在原图相邻的两个格子之间插 stride-1 个 0（这样原图就会变大了），padding <- kernel-padding-1\n        self.decoder = nn.Sequential(\n            nn.Linear(64, 64*4*4), # (64) -> (64*4*4)\n            nn.BatchNorm1d(64*4*4),\n            nn.ReLU(),\n            nn.Unflatten(1, (64, 4, 4)), # (64*4*4) -> (64, 4, 4)\n            nn.ConvTranspose2d(64,128,kernel_size=4,stride=2,padding=1), # (64,4,4) -> (128,8,8)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,128,kernel_size=4,stride=2,padding=1), # (128,8,8) -> (128,16,16)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,128,kernel_size=4,stride=2,padding=1), # (128,16,16) -> (128,32,32)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,64,kernel_size=4,stride=2,padding=1), # (128,32,32) -> (3,64,64)\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 16, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 3, kernel_size=3, padding=1, stride=1),\n            nn.Tanh()\n        )\n    \n    def encoder(self, x):\n        x = self.preconv(x) # (3,64,64) -> (32,64,64)\n        x = self.layer0(x) # (32,64,64) -> (64,32,32) 且通过 resnet(shortcut) 实现，下同\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x) # (128,8,8) -> (64,4,4)\n        x = self.fc(x) # (64,4,4) -> (64*4*4) -> (64)\n        return x\n        \n    def forward(self, x): # x : (3, 64, 64)\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\nsummary(ResNet(), input_size=(64, 3, 64, 64))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:27:50.231017Z","iopub.execute_input":"2024-12-24T12:27:50.231381Z","iopub.status.idle":"2024-12-24T12:27:50.311573Z","shell.execute_reply.started":"2024-12-24T12:27:50.231348Z","shell.execute_reply":"2024-12-24T12:27:50.310735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class fcn_autoencoder(nn.Module):\n    def __init__(self):\n        super(fcn_autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(64 * 64 * 3, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(), \n            nn.Linear(64, 12), \n            nn.ReLU(), \n            nn.Linear(12, 3)\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(3, 12),\n            nn.ReLU(), \n            nn.Linear(12, 64),\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(), \n            nn.Linear(128, 64 * 64 * 3), \n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nclass conv_autoencoder(nn.Module):\n    def __init__(self):\n        super(conv_autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 12, 4, stride=2, padding=1),         \n            nn.ReLU(),\n            nn.Conv2d(12, 24, 4, stride=2, padding=1),        \n            nn.ReLU(),\n\t\t\t      nn.Conv2d(24, 48, 4, stride=2, padding=1),         \n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),\n            nn.ReLU(),\n\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), \n            nn.ReLU(),\n            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 12, 4, stride=2, padding=1),            \n            nn.ReLU(),\n            nn.Conv2d(12, 24, 4, stride=2, padding=1),    \n            nn.ReLU(),\n        )\n        self.enc_out_1 = nn.Sequential(\n            nn.Conv2d(24, 48, 4, stride=2, padding=1),  \n            nn.ReLU(),\n        )\n        self.enc_out_2 = nn.Sequential(\n            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1), \n            nn.ReLU(),\n\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), \n            nn.ReLU(),\n            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1), \n            nn.Tanh(),\n        )\n\n    def encode(self, x):\n        h1 = self.encoder(x)\n        return self.enc_out_1(h1), self.enc_out_2(h1)\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if torch.cuda.is_available():\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        return self.decode(z), mu, logvar\n\n\ndef loss_vae(recon_x, x, mu, logvar, criterion):\n    \"\"\"\n    recon_x: generating images\n    x: origin images\n    mu: latent mean\n    logvar: latent log variance\n    \"\"\"\n    mse = criterion(recon_x, x)\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.sum(KLD_element).mul_(-0.5)\n    return mse + KLD","metadata":{"id":"Wi8ds1fugCkR","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:27:55.752884Z","iopub.execute_input":"2024-12-24T12:27:55.753410Z","iopub.status.idle":"2024-12-24T12:27:55.771647Z","shell.execute_reply.started":"2024-12-24T12:27:55.753375Z","shell.execute_reply":"2024-12-24T12:27:55.770678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset module\n\nModule for obtaining and processing data. The transform function here normalizes image's pixels from [0, 255] to [-1.0, 1.0].\n","metadata":{"id":"vrJ9bScg9AgO"}},{"cell_type":"code","source":"class CustomTensorDataset(TensorDataset):\n    \"\"\"TensorDataset with support of transforms.\n    \"\"\"\n    def __init__(self, tensors):\n        self.tensors = tensors\n        if tensors.shape[-1] == 3:\n            self.tensors = tensors.permute(0, 3, 1, 2)\n        \n        self.transform = transforms.Compose([\n          transforms.Lambda(lambda x: x.to(torch.float32)),\n          transforms.Lambda(lambda x: 2. * x/255. - 1.),\n        ])\n        \n    def __getitem__(self, index):\n        x = self.tensors[index]\n        \n        if self.transform:\n            # mapping images to [-1.0, 1.0]\n            x = self.transform(x)\n\n        return x\n\n    def __len__(self):\n        return len(self.tensors)","metadata":{"id":"33fWhE-h9LPq","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:28:00.504016Z","iopub.execute_input":"2024-12-24T12:28:00.504401Z","iopub.status.idle":"2024-12-24T12:28:00.511522Z","shell.execute_reply.started":"2024-12-24T12:28:00.504355Z","shell.execute_reply":"2024-12-24T12:28:00.510465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"id":"XKNUImqUhIeq"}},{"cell_type":"markdown","source":"## Configuration\n","metadata":{"id":"7ebAJdjFmS08"}},{"cell_type":"code","source":"# Training hyperparameters\nnum_epochs = 100\nbatch_size = 512\nlearning_rate = 1e-3\n\n# Build training dataloader\nx = torch.from_numpy(train)\ntrain_dataset = CustomTensorDataset(x)\n\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n\n# Model\nmodel_type = 'resnet'   # selecting a model type from {'cnn', 'fcn', 'vae', 'resnet'}\nmodel_classes = {'fcn': fcn_autoencoder(), 'cnn': conv_autoencoder(), 'vae': VAE(),'resnet':ResNet()}\nmodel = model_classes[model_type].cuda()\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nimport transformers\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer,3,num_epochs)","metadata":{"id":"in7yLfmqtZTk","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:37:17.362665Z","iopub.execute_input":"2024-12-24T12:37:17.364149Z","iopub.status.idle":"2024-12-24T12:37:19.190085Z","shell.execute_reply.started":"2024-12-24T12:37:17.364105Z","shell.execute_reply":"2024-12-24T12:37:19.189181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training loop","metadata":{"id":"wyooN-JPm8sS"}},{"cell_type":"code","source":"\nbest_loss = np.inf\nmodel.train()\n\nqqdm_train = qqdm(range(num_epochs), desc=format_str('bold', 'Description'))\nfor epoch in qqdm_train:\n    tot_loss = list()\n    for data in train_dataloader:\n\n        # ===================loading=====================\n        img = data.float().cuda()\n        if model_type in ['fcn']:\n            img = img.view(img.shape[0], -1)\n\n        # ===================forward=====================\n        output = model(img)\n        if model_type in ['vae']:\n            loss = loss_vae(output[0], img, output[1], output[2], criterion)\n        else:\n            loss = criterion(output, img)\n\n        tot_loss.append(loss.item())\n        # ===================backward====================\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    # ===================save_best====================\n    mean_loss = np.mean(tot_loss)\n    if mean_loss < best_loss:\n        best_loss = mean_loss\n        torch.save(model, 'best_model_{}.pt'.format(model_type))\n    # ===================log========================\n    qqdm_train.set_infos({\n        'epoch': f'{epoch + 1:.0f}/{num_epochs:.0f}',\n        'loss': f'{mean_loss:.4f}',\n    })\n    # ===================save_last========================\n    torch.save(model, 'last_model_{}.pt'.format(model_type))","metadata":{"id":"JoW1UrrxgI_U","outputId":"78a74d89-0605-4e57-d99c-f86cf110cda4","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:38:00.870393Z","iopub.execute_input":"2024-12-24T12:38:00.871314Z","iopub.status.idle":"2024-12-24T12:43:39.492844Z","shell.execute_reply.started":"2024-12-24T12:38:00.871277Z","shell.execute_reply":"2024-12-24T12:43:39.491764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\nModel is loaded and generates its anomaly score predictions.","metadata":{"id":"Wk0UxFuchLzR"}},{"cell_type":"markdown","source":"## Initialize\n- dataloader\n- model\n- prediction file","metadata":{"id":"evgMW3OwoGqD"}},{"cell_type":"code","source":"eval_batch_size = 200\n\n# build testing dataloader\ndata = torch.tensor(test, dtype=torch.float32)\ntest_dataset = CustomTensorDataset(data)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=eval_batch_size, num_workers=1)\neval_loss = nn.MSELoss(reduction='none')\n\n# load trained model\ncheckpoint_path = f'last_model_{model_type}.pt'\nmodel = torch.load(checkpoint_path)\nmodel.eval()\n\n# prediction file \nout_file = '/kaggle/working/prediction.csv'","metadata":{"id":"_MBnXAswoKmq","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T11:54:28.888666Z","iopub.execute_input":"2024-12-20T11:54:28.888954Z","iopub.status.idle":"2024-12-20T11:54:29.309397Z","shell.execute_reply.started":"2024-12-20T11:54:28.888928Z","shell.execute_reply":"2024-12-20T11:54:29.308281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anomality = list()\nwith torch.no_grad():\n  for i, data in enumerate(test_dataloader):\n    img = data.float().cuda()\n    if model_type in ['fcn']:\n      img = img.view(img.shape[0], -1)\n    output = model(img)\n    if model_type in ['vae']:\n      output = output[0]\n    if model_type in ['fcn']:\n        loss = eval_loss(output, img).sum(-1)\n    else:\n        loss = eval_loss(output, img).sum([1, 2, 3])\n    anomality.append(loss)\nanomality = torch.cat(anomality, axis=0)\nanomality = torch.sqrt(anomality).reshape(len(test), 1).cpu().numpy()\n\ndf = pd.DataFrame(anomality, columns=['score'])\ndf.to_csv(out_file, index_label = 'ID')","metadata":{"id":"_1IxCX2iCW6V","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T11:54:29.310801Z","iopub.execute_input":"2024-12-20T11:54:29.311095Z","iopub.status.idle":"2024-12-20T11:54:33.499101Z","shell.execute_reply.started":"2024-12-20T11:54:29.311069Z","shell.execute_reply":"2024-12-20T11:54:33.498120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 获取 score 最大和最小的四个索引\nmax_idx = df.score.nlargest(4).index\nmin_idx = df.score.nsmallest(4).index\n\n# 创建子图布局，2 行 4 列\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\n\n# 绘制最大分数的图像\nfor i, idx in enumerate(max_idx):\n    # 原始图像\n    axes[0, i].imshow(test[idx])\n    axes[0, i].set_title(f'Max Score {i+1}')\n    axes[0, i].axis('off')  # 隐藏坐标轴\n\n    # 获取预测图像\n    img = test_dataset.__getitem__(idx)\n    pred = model(img.unsqueeze(0).cuda()).squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n\n    # 对预测图像进行归一化并转换为 int 类型\n    pred = (pred + 1) * 255 / 2\n    pred = pred.astype(int)\n\n    # 显示预测图像\n    axes[1, i].imshow(pred)\n    axes[1, i].set_title(f'Predicted {i+1}')\n    axes[1, i].axis('off')  # 隐藏坐标轴\n\n# 绘制最小分数的图像\nfor i, idx in enumerate(min_idx):\n    # 原始图像\n    axes[2, i].imshow(test[idx])\n    axes[2, i].set_title(f'Min Score {i+1}')\n    axes[2, i].axis('off')  # 隐藏坐标轴\n\n    # 获取预测图像\n    img = test_dataset.__getitem__(idx)\n    pred = model(img.unsqueeze(0).cuda()).squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n\n    # 对预测图像进行归一化并转换为 int 类型\n    pred = (pred + 1) * 255 / 2\n    pred = pred.astype(int)\n\n    # 显示预测图像\n    axes[3, i].imshow(pred)\n    axes[3, i].set_title(f'Predicted {i+1}')\n    axes[3, i].axis('off')  # 隐藏坐标轴\n\n# 显示图像\nplt.tight_layout()  # 自动调整子图布局\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T11:54:33.501176Z","iopub.execute_input":"2024-12-20T11:54:33.501535Z","iopub.status.idle":"2024-12-20T11:54:34.298643Z","shell.execute_reply.started":"2024-12-20T11:54:33.501485Z","shell.execute_reply":"2024-12-20T11:54:34.297732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T11:54:34.299983Z","iopub.execute_input":"2024-12-20T11:54:34.300415Z","iopub.status.idle":"2024-12-20T11:54:34.323153Z","shell.execute_reply.started":"2024-12-20T11:54:34.300370Z","shell.execute_reply":"2024-12-20T11:54:34.322359Z"}},"outputs":[],"execution_count":null}]}